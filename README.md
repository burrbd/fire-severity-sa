# ğŸ”¥ Fire Severity Mapping â€” South Australia

[![Tests](https://github.com/burrbd/fire-severity-sa/workflows/Tests/badge.svg)](https://github.com/burrbd/fire-severity-sa/actions)
[![Coverage](https://codecov.io/gh/burrbd/fire-severity-sa/branch/main/graph/badge.svg)](https://codecov.io/gh/burrbd/fire-severity-sa)
[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An open, automated mapping pipeline to generate and publish **fire severity maps** (dNBR) for South Australian wildfires and prescribed burns.

## ğŸ¯ Purpose

Generate and publish fire severity maps for South Australian wildfires using a clean, scalable architecture with cloud storage and database persistence.

## ğŸ—ï¸ Architecture Overview

### **Job-Based Polymorphic Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Job Classes   â”‚    â”‚   Job Service    â”‚    â”‚   Publishers    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ DummyJob      â”‚â”€â”€â”€â–¶â”‚ â€¢ Store/Retrieve â”‚â”€â”€â”€â–¶â”‚ â€¢ Upload to S3  â”‚
â”‚ â€¢ GEEJob        â”‚    â”‚   jobs           â”‚    â”‚ â€¢ Generate STAC â”‚
â”‚ â€¢ execute()     â”‚    â”‚ â€¢ CRUD ops       â”‚    â”‚ â€¢ Make availableâ”‚
â”‚   returns Job   â”‚    â”‚ â€¢ Single DB      â”‚    â”‚   for maps      â”‚
â”‚                 â”‚    â”‚   access point   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Core Components:**

1. **`DNBRAnalysisJob`** - Batch job containing multiple analyses with single ULID
2. **`Job` (Abstract)** - Polymorphic job classes (DummyJob, GEEJob) with execute() method
3. **`JobService`** - Simple CRUD operations for job storage (DynamoDB)
4. **`DNBRAnalysis`** - Individual analysis objects within a job
5. **`S3AnalysisPublisher`** - Handles S3 uploads with STAC structure

### **Data Flow:**
1. **Execute Job** â†’ Creates job with multiple analyses for batch processing
2. **Store Job** â†’ Saves to DynamoDB via JobService
3. **Publish Job** â†’ Uploads all analyses to S3 with STAC metadata
4. **Map** â†’ Reads from S3 URLs for visualization

## ğŸ“ Current Implementation Status

### âœ… **Completed:**
- **Job-Based Architecture** - Polymorphic job classes with batch processing
- **DynamoDB Integration** - JobService with simple CRUD operations
- **S3 Integration** - Complete STAC-compliant publishing with job-based structure
- **GitHub Actions** - Automated job execution and publishing workflows
- **89% Test Coverage** - Comprehensive behavior-focused testing

### ğŸ”„ **In Progress:**
- **STAC API Lambda** - Serverless API for querying published data

### ğŸ“‹ **Next Steps:**
1. **STAC API Lambda** - Create serverless API for data discovery
2. **TiTiler Integration** - Dynamic COG tiling for map visualization
3. **Frontend Development** - Simple web interface for data exploration
4. **GEE Integration** - Replace dummy generator with real GEE analysis

## ğŸš€ Quick Start

### Setup
```bash
git clone https://github.com/burrbd/fire-severity-sa.git
cd fire-severity-sa
pip install -r requirements.txt
```

### AWS Configuration
```bash
# Configure AWS credentials
aws configure

# Create DynamoDB table (if not exists)
aws dynamodb create-table \
  --table-name fire-severity-jobs-dev \
  --attribute-definitions AttributeName=job_id,AttributeType=S \
  --key-schema AttributeName=job_id,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST \
  --region ap-southeast-2
```

### Run Analysis (Job-Based)
```bash
# Execute job (creates job with multiple analyses, stores in DynamoDB)
python scripts/dnbr_analysis_job.py data/dummy_data/fires.geojson dummy

# Publish job to S3 (retrieves from DynamoDB, uploads to S3 with STAC)
python scripts/publish_dnbr_job.py --job-id <JOB_ID>
```

### GitHub Actions Workflow
1. **Execute dNBR Job** â†’ Creates job with analyses
2. **Publish dNBR Job** â†’ Publishes to S3 with STAC structure

## ğŸ§ª Testing

### Run Tests
```bash
python -m pytest tests/ -v --cov=dnbr --cov-report=term-missing
```

### Pre-commit Hooks
Tests run automatically before commits. Set up with:
```bash
./setup-pre-commit.sh
```

## ğŸ“ Project Structure

```
fire-severity-sa/
â”œâ”€â”€ dnbr/                   # Core domain logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ analysis.py         # DNBRAnalysis class (individual analyses)
â”‚   â”œâ”€â”€ job.py              # DNBRAnalysisJob class (batch jobs)
â”‚   â”œâ”€â”€ jobs.py             # Polymorphic Job classes (DummyJob, GEEJob)
â”‚   â”œâ”€â”€ job_service.py      # DynamoDB operations for jobs
â”‚   â”œâ”€â”€ publisher.py        # S3 publishing with STAC structure
â”‚   â””â”€â”€ fire_metadata.py    # Fire metadata extraction
â”œâ”€â”€ scripts/                # GitHub Actions entry points
â”‚   â”œâ”€â”€ dnbr_analysis_job.py    # Job execution (replaces generate_dnbr_analysis.py)
â”‚   â”œâ”€â”€ publish_dnbr_job.py     # Job publishing (replaces publish_dnbr_analysis.py)
â”‚   â”œâ”€â”€ generate_map_shell.py   # Map generation
â”‚   â””â”€â”€ generate_dnbr_utils.py  # Raster processing utilities
â”œâ”€â”€ data/                   # Input data
â”‚   â””â”€â”€ dummy_data/         # Test data
â”‚       â”œâ”€â”€ fires.geojson   # Multiple AOIs (2 features)
â”‚       â”œâ”€â”€ fire.geojson    # Single AOI (1 feature)
â”‚       â””â”€â”€ raw_dnbr.tif    # Dummy raster data
â”œâ”€â”€ docs/                   # Documentation and outputs
â”‚   â”œâ”€â”€ index.html         # GitHub Pages site
â”‚   â””â”€â”€ outputs/           # Generated outputs
â”œâ”€â”€ .github/               # GitHub Actions
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ execute_dnbr_job.yml      # Job execution workflow
â”‚       â”œâ”€â”€ publish_dnbr_job.yml      # Job publishing workflow
â”‚       â”œâ”€â”€ generate_map_shell.yml    # Map generation
â”‚       â””â”€â”€ tests.yml                 # Test suite
â”œâ”€â”€ tests/                 # Test suite
â”‚   â”œâ”€â”€ test_analysis.py              # DNBRAnalysis tests
â”‚   â”œâ”€â”€ test_job_execution.py         # Job execution tests
â”‚   â”œâ”€â”€ test_job_service_integration.py # JobService integration tests
â”‚   â”œâ”€â”€ test_job_status.py            # Job status tests
â”‚   â”œâ”€â”€ test_publisher.py             # Publisher tests
â”‚   â”œâ”€â”€ test_s3_integration.py        # S3 integration tests
â”‚   â””â”€â”€ test_scripts.py               # Script tests
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ pytest.ini           # Test configuration
â””â”€â”€ README.md
```

## ğŸ”„ Development Workflow

### **Current Architecture:**
1. **Job Execution** â†’ Creates job with multiple analyses, stores in DynamoDB
2. **Job Publishing** â†’ Retrieves job, uploads analyses to S3 with STAC
3. **Map Generation** â†’ Reads from S3 URLs for visualization

### **S3 Structure:**
```
s3://bucket/
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ {job_id}/
â”‚       â”œâ”€â”€ {aoi_id}/
â”‚       â”‚   â”œâ”€â”€ {aoi_id}_dnbr.cog.tif
â”‚       â”‚   â””â”€â”€ {aoi_id}_aoi.geojson
â”‚       â””â”€â”€ {aoi_id2}/
â”‚           â”œâ”€â”€ {aoi_id2}_dnbr.cog.tif
â”‚           â””â”€â”€ {aoi_id2}_aoi.geojson
â””â”€â”€ stac/
    â”œâ”€â”€ collections/
    â”‚   â””â”€â”€ fires.json
    â””â”€â”€ items/
        â””â”€â”€ {job_id}/
            â”œâ”€â”€ {aoi_id}.json
            â””â”€â”€ {aoi_id2}.json
```

## ğŸ”§ Technical Details

### **DynamoDB Schema:**
```json
{
  "job_id": "string (ULID)",
  "generator_type": "string (dummy|gee)",
  "created_at": "string (ISO timestamp)",
  "updated_at": "string (ISO timestamp)",
  "analysis_count": "number",
  "analyses": "string (JSON array of analysis data)"
}
```

### **Job Types:**
- **`DummyJob`** - Synchronous test/development job with immediate completion
- **`GEEJob`** - Asynchronous Google Earth Engine job with pending status

### **Analysis Status:**
- **`PENDING`** - Analysis created, waiting for processing (GEE)
- **`COMPLETED`** - Analysis finished, ready for publishing
- **`FAILED`** - Analysis failed, needs investigation

### **AWS Authentication:**
- GitHub Actions uses AWS credentials from repository secrets
- Local development uses AWS CLI configuration
- IAM user requires DynamoDB and S3 permissions

## ğŸš€ Next Steps

### **Immediate:**
1. **STAC API Lambda** - Serverless API for data discovery
2. **TiTiler Integration** - Dynamic COG tiling for map visualization
3. **Simple Frontend** - Web interface for data exploration

### **Medium Term:**
1. **GEE Integration** - Real Google Earth Engine analysis
2. **Production Deployment** - Move from development to production
3. **Monitoring & Alerting** - Job status monitoring and notifications

### **Long Term:**
1. **Multi-Provider Support** - Support for different data providers
2. **Advanced Analytics** - Additional fire severity metrics
3. **Real-time Processing** - Near real-time fire severity mapping

## ğŸ“ License

MIT License - see LICENSE file for details.
